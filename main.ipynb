{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Original Data Set and Creating Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# original_df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# # Selecting only ARTICLE_ID and SECTION_TEXT columns\n",
    "# subset_df = original_df[['ARTICLE_ID', 'SECTION_TEXT']]\n",
    "\n",
    "# # Taking a subset of 2000 rows from the original dataset\n",
    "# subset_df = subset_df.head(2000)\n",
    "\n",
    "# subset_df.to_csv(\"subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and Pre Processing Data from Sample (subset.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import OrderedDict\n",
    "\n",
    "subset_df = pd.read_csv(\"subset.csv\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Tokenizing the text\n",
    "    words = word_tokenize(text)\n",
    "    # Removing stopwords and punctuation, and converting to lowercase\n",
    "    filtered_words = [word.lower() for word in words if word.isalnum()]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "# Dictionary to store unique words with index\n",
    "corpus = OrderedDict()\n",
    "\n",
    "# Iterate through the rows to build the corpus\n",
    "for index, row in subset_df.iterrows():\n",
    "    article_text = row['SECTION_TEXT']\n",
    "    preprocessed_words = preprocess_text(article_text)\n",
    "    # Adding preprocessed words to the corpus\n",
    "    for word in preprocessed_words:\n",
    "        if word not in corpus:\n",
    "            corpus[word] = len(corpus)  # Assign index if the word is new\n",
    "\n",
    "\n",
    "# print(\"Corpus with assigned indices:\")\n",
    "# for idx, word in corpus.items():\n",
    "#     print(f\"({word}, '{idx}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store TF for each word in the corpus\n",
    "term_frequency = {index: 0 for index in corpus.values()}\n",
    "\n",
    "# Iterating through the rows to calculate term frequency\n",
    "for index, row in subset_df.iterrows():\n",
    "    article_text = row['SECTION_TEXT']\n",
    "    preprocessed_words = preprocess_text(article_text)\n",
    "\n",
    "    # Dictionary to store TF in articles\n",
    "    tf_article = {index: 0 for index in corpus.values()}\n",
    "\n",
    "    # TF for each word in the article\n",
    "    for word in preprocessed_words:\n",
    "        if word in corpus:\n",
    "            index = corpus[word]\n",
    "            tf_article[index] += 1\n",
    "            term_frequency[index] += 1\n",
    "\n",
    "    # Remove words with 0 frequency\n",
    "    non_zero_tf_article = {word: tf for word,\n",
    "                           tf in tf_article.items() if tf > 0}\n",
    "#     print(f\"Article {row['ARTICLE_ID']} TF: {non_zero_tf_article}\")\n",
    "\n",
    "# term_frequency = {index: tf for index, tf in term_frequency.items() if tf > 0}\n",
    "\n",
    "\n",
    "# print(\"\\nNon-zero Term Frequencies:\")\n",
    "# print(term_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#  an array to store inverse document frequency \n",
    "df_array = np.zeros(len(corpus))\n",
    "\n",
    "# Iterate through the rows to calculate IDF\n",
    "for index, row in subset_df.iterrows():\n",
    "    article_text = row['SECTION_TEXT']\n",
    "    preprocessed_words = preprocess_text(article_text)\n",
    "\n",
    "    # Track whether a word has appeared in the current article\n",
    "    appeared_words = set()\n",
    "\n",
    "    # Count the appearance of each word in the current article\n",
    "    for word in preprocessed_words:\n",
    "        if word in corpus and word not in appeared_words:\n",
    "            word_index = corpus[word]\n",
    "            df_array[word_index] += 1\n",
    "            appeared_words.add(word)\n",
    "\n",
    "# Print the document frequency for each word with its index\n",
    "# print(\"Inverse Document Frequency (IDF):\")\n",
    "# for word_idx, doc_freq in enumerate(df_array):\n",
    "#     print(f\"({word_idx}, {doc_freq})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF / IDF Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store weights\n",
    "weights = {}\n",
    "\n",
    "num_documents = len(subset_df)\n",
    "\n",
    "\n",
    "for index, row in subset_df.iterrows():\n",
    "\n",
    "    article_text = row['SECTION_TEXT']\n",
    "\n",
    "    preprocessed_words = preprocess_text(article_text)\n",
    "\n",
    "    tfidf_article = {}\n",
    "    appeared_words = set()\n",
    "\n",
    "    for word in preprocessed_words:\n",
    "\n",
    "        if word in corpus and word not in appeared_words:\n",
    "\n",
    "            word_index = corpus[word]\n",
    "\n",
    "            tf = preprocessed_words.count(word)\n",
    "\n",
    "            idf = np.log(num_documents / (1 + df_array[word_index]))\n",
    "\n",
    "            tfidf = tf * idf\n",
    "\n",
    "            tfidf_article[word_index] = round(\n",
    "\n",
    "                tfidf)  # Round to nearest integer\n",
    "\n",
    "            appeared_words.add(word)\n",
    "\n",
    "    weights[index] = tfidf_article\n",
    "\n",
    "\n",
    "# Print TF/IDF weights\n",
    "\n",
    "# print(\"TF/IDF Weights:\")\n",
    "\n",
    "# for article_id, tfidf in weights.items():\n",
    "\n",
    "#     print(f\"Article {article_id}: {tfidf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "#  Calculating TF-IDF score for each word in the query based on the corpus document frequency.\n",
    "def calculate_tf_idf_query(query, corpus, df_array, num_documents):\n",
    "  \n",
    "    preprocessed_query = preprocess_text(query)\n",
    "    tf_query = {word: 0 for word in corpus}\n",
    "    for word in preprocessed_query:\n",
    "        if word in corpus:\n",
    "            tf_query[word] += 1\n",
    "\n",
    "    tf_idf_query = {}\n",
    "    for word, tf in tf_query.items():\n",
    "        if tf > 0 and word in corpus:\n",
    "            word_index = corpus[word]\n",
    "            idf = np.log(num_documents / (1 + df_array[word_index]))\n",
    "            tf_idf_query[word_index] = tf * idf\n",
    "\n",
    "    return tf_idf_query\n",
    "\n",
    "\n",
    "# Calculate the cosine similarity between the document vector and the query vector.\n",
    "def calculate_cosine_similarity(doc_vector, query_vector):\n",
    "  \n",
    "    dot_product = sum([doc_vector[word_index] * query_vector.get(word_index, 0)\n",
    "                      for word_index in doc_vector])\n",
    "    doc_vector_magnitude = math.sqrt(\n",
    "        sum([val**2 for val in doc_vector.values()]))\n",
    "    query_vector_magnitude = math.sqrt(\n",
    "        sum([val**2 for val in query_vector.values()]))\n",
    "\n",
    "    if doc_vector_magnitude == 0 or query_vector_magnitude == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        cosine_similarity = dot_product / \\\n",
    "            (doc_vector_magnitude * query_vector_magnitude)\n",
    "        return cosine_similarity\n",
    "\n",
    "\n",
    "query = input(\"Enter your query statement: \")\n",
    "tf_idf_query = calculate_tf_idf_query(query, corpus, df_array, num_documents)\n",
    "\n",
    "# Calculating relevance scores\n",
    "relevance_scores = {}\n",
    "for doc_id, doc_vector in weights.items():\n",
    "    relevance_scores[doc_id] = calculate_cosine_similarity(\n",
    "        doc_vector, tf_idf_query)\n",
    "\n",
    "# Ranking documents on their relevance scores\n",
    "ranked_docs = sorted(relevance_scores.items(),\n",
    "                     key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nRanked Documents (by relevance):\")\n",
    "for doc_id, score in ranked_docs:\n",
    "    print(f\"Document {doc_id}: {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
