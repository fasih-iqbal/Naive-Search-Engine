{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "nltk.download('wordnet')\n",
    "# Load your subset dataframe\n",
    "file_path = r\"C:\\Users\\PC\\Documents\\Symmester 4\\Big Data\\Assignment 2/subset.csv\"\n",
    "#subset_df = pd.read_csv(file_path)\n",
    "#to avoid dtype warning reads all as str\n",
    "subset_df = pd.read_csv(file_path, dtype=str) \n",
    "\n",
    "\n",
    "# Initialize WordNet Lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Tokenize the text\n",
    "        words = word_tokenize(text)\n",
    "        # Remove stop words and perform lemmatization\n",
    "        filtered_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "# Create a corpus\n",
    "corpus = OrderedDict()\n",
    "\n",
    "# Iterate through the rows to build the corpus\n",
    "for index, row in subset_df.iterrows():\n",
    "    article_text = row['SECTION_TEXT']\n",
    "    preprocessed_words = preprocess_text(article_text)\n",
    "    for word in preprocessed_words:\n",
    "        if word not in corpus:\n",
    "            corpus[word] = len(corpus)\n",
    "\n",
    "# Print all words in the vocabulary along with their index\n",
    "# for index, word in corpus.items():\n",
    "#     print(f\"({index}, '{word}'), \", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a dictionary to store TF for each article\n",
    "tf_dict = {}\n",
    "\n",
    "# Iterate through the rows to calculate TF\n",
    "for index, row in subset_df.iterrows():\n",
    "    article_text = row['SECTION_TEXT']\n",
    "    preprocessed_words = preprocess_text(article_text)\n",
    "    article_tf = {idx: 0 for idx in corpus.values()}\n",
    "    for word in preprocessed_words:\n",
    "        if word in corpus:\n",
    "            article_tf[corpus[word]] += 1\n",
    "    article_tf = {idx: freq for idx, freq in article_tf.items() if freq > 0}\n",
    "    tf_dict[index] = article_tf\n",
    "\n",
    "\n",
    "# Print TF for each article\n",
    "# for article_id, tf in tf_dict.items():\n",
    "#     print(f\"\\n{article_id}\")\n",
    "#     for term_idx, tf_value in tf.items():\n",
    "#         print(f\"({term_idx}, {tf_value}), \", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an array to store the document frequency (DF) for each word\n",
    "df_array = np.zeros(len(corpus))\n",
    "\n",
    "# Iterate through the TF dictionary to calculate DF\n",
    "for tf in tf_dict.values():\n",
    "    for word_idx in tf.keys():\n",
    "        df_array[word_idx] += 1\n",
    "\n",
    "# Print index and DF for each word\n",
    "# for idx, df_value in enumerate(df_array):\n",
    "#     print(f\"({idx}, {df_value}), \", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF/DF weights\n",
    "tf_df_weights = {}\n",
    "for article_id, article_tf in tf_dict.items():\n",
    "    tf_df_weights[article_id] = {term_idx: tf_value * df_array[term_idx] for term_idx, tf_value in article_tf.items()}\n",
    "\n",
    "# Print TF/DF for each document\n",
    "# for article_id, tfdf in tf_df_weights.items():\n",
    "#     print(f\"{article_id}\")\n",
    "#     for term_idx, tfdf_value in tfdf.items():\n",
    "#         print(f\"({term_idx}, {tfdf_value}), \", end=\"\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 2D array to store document vectors\n",
    "document_vectors = []\n",
    "\n",
    "# Iterate through the TF dictionary to create vectors for each document\n",
    "for tfidf in tf_dict.values():\n",
    "    document_vector = [0] * len(corpus)\n",
    "    for word_idx, tfidf_value in tfidf.items():\n",
    "        document_vector[word_idx] = tfidf_value\n",
    "    document_vectors.append(document_vector)\n",
    "\n",
    "\n",
    "\n",
    "# Print the first two document vectors to check\n",
    "# for i in range(2):\n",
    "#     print(f\"Document Vector {i+1}: {document_vectors[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate relevance between query vector and document vector\n",
    "def calculate_relevance_sparse(query_vector, document_vector):\n",
    "    relevance = 0\n",
    "    for term_idx in query_vector:\n",
    "        if term_idx in document_vector:\n",
    "            relevance += query_vector[term_idx] * document_vector[term_idx]\n",
    "    return relevance\n",
    "\n",
    "# Function to calculate relevance between query vector and document vector\n",
    "def calculate_relevance(query_vector, document_vector):\n",
    "    relevance = sum(qi * di for qi, di in zip(query_vector, document_vector))\n",
    "    return relevance\n",
    "\n",
    "# Get user input for the query text\n",
    "query_text = input(\"Enter your query text: \")\n",
    "\n",
    "# Preprocess the query text\n",
    "preprocessed_query = preprocess_text(query_text)\n",
    "\n",
    "# Initialize a dictionary to store TF for the query\n",
    "query_tf = {idx: 0 for idx in corpus.values()}\n",
    "\n",
    "# Calculate TF for the query\n",
    "for word in preprocessed_query:\n",
    "    if word in corpus:\n",
    "        query_tf[corpus[word]] += 1\n",
    "\n",
    "# Create the query vector (sparse representation)\n",
    "query_vector_sparse = {idx: tf_value for idx, tf_value in query_tf.items() if tf_value != 0}\n",
    "\n",
    "# Initialize a dictionary to store relevance scores along with document index\n",
    "document_relevance_sparse = {}\n",
    "\n",
    "# Initialize document_vectors_sparse dictionary\n",
    "document_vectors_sparse = {}\n",
    "\n",
    "# Iterate through the TF dictionary to create sparse vectors for each document\n",
    "for idx, doc_vector in enumerate(document_vectors):\n",
    "    doc_sparse_vector = {term_idx: tfidf_value for term_idx, tfidf_value in enumerate(doc_vector) if tfidf_value != 0}\n",
    "    document_vectors_sparse[idx] = doc_sparse_vector\n",
    "\n",
    "# Calculate relevance between query vector and each document vector (using sparse representation)\n",
    "for doc_id, doc_vector in document_vectors_sparse.items():\n",
    "    relevance_sparse = calculate_relevance_sparse(query_vector_sparse, doc_vector)\n",
    "    document_relevance_sparse[doc_id] = relevance_sparse\n",
    "\n",
    "\n",
    "# print(\"Query Vector:\", query_vector_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 relevant documents:\n",
      "doc 212: 5.00\n",
      "doc 236: 5.00\n",
      "doc 1063: 5.00\n",
      "doc 3: 4.00\n",
      "doc 6: 3.00\n",
      "doc 51: 3.00\n",
      "doc 402: 3.00\n",
      "doc 489: 3.00\n",
      "doc 618: 3.00\n",
      "doc 690: 3.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate relevance between query vector and each document vector (using full representation)\n",
    "document_relevance = []\n",
    "\n",
    "for doc_vector in document_vectors:\n",
    "    relevance = calculate_relevance(list(query_vector_sparse.values()), doc_vector)\n",
    "    document_relevance.append(relevance)\n",
    "\n",
    "# Sort the document relevance dictionaries by relevance score\n",
    "sorted_document_relevance_sparse = sorted(document_relevance_sparse.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_document_relevance = sorted(enumerate(document_relevance), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the top 10 relevant documents\n",
    "print(\"Top 10 relevant documents:\")\n",
    "for i, (doc_id, relevance) in enumerate(sorted_document_relevance_sparse[:10], start=1):\n",
    "    print(f\"doc {doc_id + 1}: {relevance:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
